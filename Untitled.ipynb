{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13852b86-17d1-46f5-94bd-9b25d8bd514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import misc\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.layers import (Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, \n",
    "                          Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout)\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a23c50-7464-4d16-a796-6bdd7d097618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 720 images belonging to 9 classes.\n",
      "Found 180 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"./data/images_original/\"\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,target_size=(228,432),color_mode=\"rgba\",class_mode='categorical',batch_size=30)\n",
    "\n",
    "validation_dir = \"./data/images_original_test/\"\n",
    "vali_datagen = ImageDataGenerator(rescale=1./255)\n",
    "vali_generator = vali_datagen.flow_from_directory(validation_dir,target_size=(228,432),color_mode='rgba',class_mode='categorical',batch_size=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37d8589-d2b3-44fa-9ee0-5373cdff2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3),input_shape=(228,432,4), activation=\"relu\"),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation=\"relu\"),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation=\"relu\"),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(256, (3,3), activation=\"relu\"),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.3),\n",
    "    Dense(9, activation=\"softmax\")\n",
    "    \n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16baccc4-a650-4625-ba1f-edacae659ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 17s 714ms/step - loss: 0.0187 - accuracy: 0.9972 - val_loss: 2.6451 - val_accuracy: 0.6000\n",
      "Epoch 8/30\n",
      "24/24 [==============================] - 18s 722ms/step - loss: 0.0203 - accuracy: 0.9972 - val_loss: 2.5830 - val_accuracy: 0.6056\n",
      "Epoch 9/30\n",
      "24/24 [==============================] - 18s 733ms/step - loss: 0.0164 - accuracy: 0.9972 - val_loss: 2.6471 - val_accuracy: 0.6000\n",
      "Epoch 10/30\n",
      "24/24 [==============================] - 17s 721ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 2.7530 - val_accuracy: 0.5667\n",
      "Epoch 11/30\n",
      "24/24 [==============================] - 17s 707ms/step - loss: 0.0129 - accuracy: 0.9972 - val_loss: 2.8768 - val_accuracy: 0.5833\n",
      "Epoch 12/30\n",
      "24/24 [==============================] - 18s 723ms/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 2.8615 - val_accuracy: 0.6056\n",
      "Epoch 13/30\n",
      "24/24 [==============================] - 18s 731ms/step - loss: 0.0193 - accuracy: 0.9958 - val_loss: 2.5484 - val_accuracy: 0.5778\n",
      "Epoch 14/30\n",
      "24/24 [==============================] - 17s 718ms/step - loss: 0.0497 - accuracy: 0.9847 - val_loss: 2.9156 - val_accuracy: 0.5667\n",
      "Epoch 15/30\n",
      "24/24 [==============================] - 17s 704ms/step - loss: 0.1494 - accuracy: 0.9458 - val_loss: 2.6571 - val_accuracy: 0.4889\n",
      "Epoch 16/30\n",
      "24/24 [==============================] - 17s 720ms/step - loss: 0.1819 - accuracy: 0.9500 - val_loss: 2.7787 - val_accuracy: 0.5556\n",
      "Epoch 17/30\n",
      "24/24 [==============================] - 17s 719ms/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 2.3883 - val_accuracy: 0.5889\n",
      "Epoch 18/30\n",
      "24/24 [==============================] - 17s 719ms/step - loss: 0.0266 - accuracy: 0.9931 - val_loss: 2.5076 - val_accuracy: 0.6167\n",
      "Epoch 19/30\n",
      "24/24 [==============================] - 17s 707ms/step - loss: 0.0114 - accuracy: 0.9958 - val_loss: 2.6692 - val_accuracy: 0.5833\n",
      "Epoch 20/30\n",
      "24/24 [==============================] - 18s 730ms/step - loss: 0.0320 - accuracy: 0.9903 - val_loss: 2.7279 - val_accuracy: 0.5944\n",
      "Epoch 21/30\n",
      "24/24 [==============================] - 18s 723ms/step - loss: 0.0226 - accuracy: 0.9972 - val_loss: 2.8384 - val_accuracy: 0.5667\n",
      "Epoch 22/30\n",
      "24/24 [==============================] - 18s 730ms/step - loss: 0.0164 - accuracy: 0.9972 - val_loss: 2.4804 - val_accuracy: 0.5722\n",
      "Epoch 23/30\n",
      "24/24 [==============================] - 17s 707ms/step - loss: 0.0128 - accuracy: 0.9972 - val_loss: 2.5433 - val_accuracy: 0.5611\n",
      "Epoch 24/30\n",
      "24/24 [==============================] - 18s 723ms/step - loss: 0.0079 - accuracy: 0.9972 - val_loss: 2.7408 - val_accuracy: 0.5722\n",
      "Epoch 25/30\n",
      "24/24 [==============================] - 18s 725ms/step - loss: 0.0060 - accuracy: 0.9986 - val_loss: 2.6650 - val_accuracy: 0.5722\n",
      "Epoch 26/30\n",
      "24/24 [==============================] - 18s 723ms/step - loss: 0.0152 - accuracy: 0.9972 - val_loss: 2.5485 - val_accuracy: 0.5722\n",
      "Epoch 27/30\n",
      "24/24 [==============================] - 17s 709ms/step - loss: 0.0101 - accuracy: 0.9986 - val_loss: 2.4261 - val_accuracy: 0.6056\n",
      "Epoch 28/30\n",
      "24/24 [==============================] - 18s 726ms/step - loss: 0.0099 - accuracy: 0.9986 - val_loss: 2.4942 - val_accuracy: 0.6111\n",
      "Epoch 29/30\n",
      "24/24 [==============================] - 18s 722ms/step - loss: 0.0116 - accuracy: 0.9986 - val_loss: 2.6695 - val_accuracy: 0.5889\n",
      "Epoch 30/30\n",
      "24/24 [==============================] - 18s 729ms/step - loss: 0.0103 - accuracy: 0.9986 - val_loss: 2.5766 - val_accuracy: 0.5667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x230d99e7d90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,validation_data=vali_generator, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fafb578f-dc85-420f-b6ac-793481b2d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelcnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "234ede44-9719-459a-8d61-125cd70dfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # No pictures displayed \n",
    "import pylab\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "path = \"./data/genres_original/reggae/reggae.00000.wav\"\n",
    "sig, fs = librosa.load(path)   \n",
    "# make pictures name \n",
    "save_path = 'test.jpg'\n",
    "\n",
    "pylab.axis('off') # no axis\n",
    "pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\n",
    "S = librosa.feature.melspectrogram(y=sig, sr=fs)\n",
    "librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\n",
    "pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78e3a7b3-c80a-450a-bc14-5ba8138eb06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"test.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2BGRA)\n",
    "img = cv2.resize(img, (228,432))\n",
    "img_array = np.array(img).flatten()\n",
    "\n",
    "img_array = img_array/255\n",
    "img_array = img_array.reshape(1,228,432,4)\n",
    "print(np.argmax(model.predict(img_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d709e4-f1d9-4f0c-bed2-af2afaaace0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
